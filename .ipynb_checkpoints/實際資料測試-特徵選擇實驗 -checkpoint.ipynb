{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac16c53",
   "metadata": {},
   "source": [
    "## 資料集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae60c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = []\n",
    "x_set = []\n",
    "y_set = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2af290",
   "metadata": {},
   "source": [
    "## churn.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a75bbdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022 17:39:14 INFO Fit Columns Type Already Success!!!\n",
      "2022 17:39:14 INFO Transform Columns Type Success!!!\n",
      "2022 17:39:14 INFO scaler Success!!!\n",
      "2022 17:39:14 INFO dummies Success!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 87)\n",
      "Counter({0: 4293, 1: 707})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "data = pd.read_csv('./data/churn.all.csv')\n",
    "\n",
    "#刪除不要的欄位\n",
    "data.drop(['id'], inplace=True, axis=1)\n",
    "data.drop(['phone_number'], inplace=True, axis=1)\n",
    "\n",
    "#將目標屬性轉為數字\n",
    "import numpy as np\n",
    "data['Churn'] = np.where(data['Churn'] == ' False', 0, 1)\n",
    "\n",
    "#分割輸入屬性及目標屬性\n",
    "\n",
    "#輸入屬性\n",
    "x_ = data.drop(['Churn'], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = data['Churn']\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "        ('define_column_type', define_column_type()),\n",
    "        ('scaler', scaler()),\n",
    "        ('dummies', dummies()),\n",
    "        #('filter', feature_selection(method = 'anova_kf', p_threshold = 1)),\n",
    "        #('sfs',feature_selection(method = 'sfs', sfs_direction = direction, sfs_scoring = scoring, sfs_cv = cv, sfs_estimator = estimator))\n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('churn.all')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056a462",
   "metadata": {},
   "source": [
    "## bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "data = pd.read_csv('./data/bank.csv')\n",
    "\n",
    "#刪除ID\n",
    "#data.drop(['id'], inplace=True, axis=1)\n",
    "\n",
    "#將目標屬性轉為數字\n",
    "import numpy as np\n",
    "data['y'] = np.where(data['y'] == 'no', 0, 1)\n",
    "\n",
    "#分割輸入屬性及目標屬性\n",
    "\n",
    "#輸入屬性\n",
    "x_ = data.drop(['y'], axis=1)#同上方法刪除行\n",
    "#x_ = data.drop(['phone_number'], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = data['y']\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "#         ('Wrong_Value',wrong_value_fillna(wrong_value=['unknown'])),\n",
    "#         ('Null_Value', xgb_fill()),\n",
    "        ('define_column_type', define_column_type()),\n",
    "        ('scaler', scaler()),\n",
    "        ('dummies', dummies()),\n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('bank')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6aec7",
   "metadata": {},
   "source": [
    "## WPBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "wpbc = pd.read_csv('./data/wpbc.csv', header=None)\n",
    "\n",
    "#刪除ID\n",
    "wpbc.drop([0], inplace=True, axis=1)\n",
    "\n",
    "#將目標屬性轉為數字\n",
    "import numpy as np\n",
    "wpbc[1] = np.where(wpbc[1] == 'N', 0, 1)\n",
    "\n",
    "#分割輸入屬性及目標屬性\n",
    "\n",
    "#輸入屬性\n",
    "x_ = wpbc.drop([1], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = wpbc[1]\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "         ('Wrong_Value',wrong_value_fillna(wrong_value=['?'])),\n",
    "         ('column_type', define_column_type()),\n",
    "         #('Outlier', fix_outlier(how='quartile')), \n",
    "         \n",
    "         ('Null_Value', xgb_fill()),\n",
    "         #('num_log', num_log()),\n",
    "         \n",
    "         ('Wrong_Value2',wrong_value_fillna(wrong_value=[np.nan, float('-inf')])),\n",
    "         ('Null_Value2', xgb_fill()),\n",
    "  \n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('WPBC')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980986c7",
   "metadata": {},
   "source": [
    "## 2.Statlog (Australian Credit Approval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#澳大利亞信貸審批，沒說目標屬性哪個是通過不通過\n",
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "data = pd.read_csv('./data/2.australian.csv', header=None)\n",
    "\n",
    "#輸入屬性\n",
    "x_ = data.drop([14], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = data[14]\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "        ('define_column_type', define_column_type()),\n",
    "        ('filter', feature_selection(method = 'var')),\n",
    "        ('scaler', scaler()),\n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('Statlog')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daada69",
   "metadata": {},
   "source": [
    "## 6.default of credit card clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#客戶信用卡資料，下個月是否付款\n",
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "data = pd.read_csv('./data/6.default of credit card clients.csv')\n",
    "\n",
    "#輸入屬性\n",
    "x_ = data.drop(['default payment next month'], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = data['default payment next month']\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "        ('define_column_type', define_column_type()),\n",
    "        ('scaler', scaler()),\n",
    "        ('dummies', dummies()),\n",
    "        #('filter', feature_selection(method = 'anova_kf', p_threshold = 1)),\n",
    "        #('sfs',feature_selection(method = 'sfs', sfs_direction = direction, sfs_scoring = scoring, sfs_cv = cv, sfs_estimator = estimator))\n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('default of credit card clients')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bfa97",
   "metadata": {},
   "source": [
    "## 9.hepatitis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db50fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#病人資訊（肝炎）->是否死\n",
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "data = pd.read_csv('./data/9.hepatitis.csv', header=None)\n",
    "\n",
    "#將目標屬性轉為數字\n",
    "import numpy as np\n",
    "data[0] = np.where(data[0] == 2, 0, 1)\n",
    "\n",
    "#輸入屬性\n",
    "x_ = data.drop([0], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = data[0]\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "        ('Wrong_Value',wrong_value_fillna(wrong_value=['?'])),\n",
    "        ('filter', feature_selection(method = 'var')),\n",
    "        ('Null_Value', xgb_fill()),\n",
    "        ('define_column_type', define_column_type()),\n",
    "#         ('dummies', dummies()),\n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('hepatitis')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba978b",
   "metadata": {},
   "source": [
    "## 11.ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#電離層的雷達回波分類好壞\n",
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "data = pd.read_csv('./data/11.ionosphere.csv', header=None)\n",
    "\n",
    "#刪除不要的欄位\n",
    "data.drop([0], inplace=True, axis=1)\n",
    "data.drop([1], inplace=True, axis=1)\n",
    "# print(data)\n",
    "\n",
    "#將目標屬性轉為數字\n",
    "import numpy as np\n",
    "data[34] = np.where(data[34] == 'g', 1, 0)\n",
    "\n",
    "#輸入屬性\n",
    "x_ = data.drop([34], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = data[34]\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "        ('define_column_type', define_column_type()),\n",
    "#         ('dummies', dummies()),\n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('ionosphere')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36011a18",
   "metadata": {},
   "source": [
    "## 16.sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分類聲納訊號掃到的是岩石（Ｒ）或金屬圓柱（Ｍ）\n",
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "data = pd.read_csv('./data/16.sonar.csv', header=None)\n",
    "\n",
    "#將目標屬性轉為數字\n",
    "import numpy as np\n",
    "data[60] = np.where(data[60] == 'R', 0, 1)\n",
    "\n",
    "#輸入屬性\n",
    "x_ = data.drop([60], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = data[60]\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "        ('define_column_type', define_column_type()),\n",
    "#         ('dummies', dummies()),\n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('sonar')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff60122",
   "metadata": {},
   "source": [
    "## 17.tic-tac-toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#井字遊戲，positive (i.e., wins for \"x\")\n",
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "data = pd.read_csv('./data/17.tic-tac-toe.csv', header=None)\n",
    "\n",
    "#將目標屬性轉為數字\n",
    "import numpy as np\n",
    "data[9] = np.where(data[9] == 'negative', 0, 1)\n",
    "\n",
    "#輸入屬性\n",
    "x_ = data.drop([9], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = data[9]\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "        ('define_column_type', define_column_type()),\n",
    "        ('dummies', dummies()),\n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('tic-tac-toe')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49d9a6",
   "metadata": {},
   "source": [
    "## 21.broadband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#電信客戶流失\n",
    "import pandas as pd\n",
    "\n",
    "#放在不同資料夾用絕對路徑\n",
    "data = pd.read_csv('./data/21.broadband.csv')\n",
    "\n",
    "#刪除不要的欄位\n",
    "data.drop(['CUST_ID'], inplace=True, axis=1)\n",
    "\n",
    "#輸入屬性\n",
    "x_ = data.drop(['BROADBAND'], axis=1)#同上方法刪除行\n",
    "\n",
    "#目標屬性\n",
    "y_ = data['BROADBAND']\n",
    "\n",
    "#去除錯誤值及空值填補\n",
    "from sklearn.pipeline import Pipeline\n",
    "from data_preprocessing_pipeline import *\n",
    "from feature_engineering import *\n",
    "\n",
    "pipe = Pipeline(\n",
    "       [\n",
    "        ('define_column_type', define_column_type()),\n",
    "        ('dummies', dummies()),\n",
    "       ])\n",
    "\n",
    "x_ = pipe.fit_transform(x_, y_)\n",
    "print(x_.shape)\n",
    "\n",
    "import collections\n",
    "c = collections.Counter(y_)\n",
    "print(c)\n",
    "\n",
    "data_name.append('broadband')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbea2db",
   "metadata": {},
   "source": [
    "## 用產生資料測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dfb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "x_, y_ = make_classification(n_samples=5000, n_features=100,\n",
    "                             n_informative=20, n_redundant=20,\n",
    "                             n_classes=2, random_state=0, shuffle=True)\n",
    "\n",
    "x_ = pd.DataFrame(x_)\n",
    "y_ = pd.DataFrame(y_)\n",
    "\n",
    "data_name.append('Manual F100_I20_R20')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "x_, y_ = make_classification(n_samples=5000, n_features=100,\n",
    "                             n_informative=40, n_redundant=20,\n",
    "                             n_classes=2, random_state=0, shuffle=True)\n",
    "\n",
    "x_ = pd.DataFrame(x_)\n",
    "y_ = pd.DataFrame(y_)\n",
    "\n",
    "data_name.append('Manual F100_I40_R20')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b518476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "x_, y_ = make_classification(n_samples=5000, n_features=100,\n",
    "                             n_informative=60, n_redundant=20,\n",
    "                             n_classes=2, random_state=0, shuffle=True)\n",
    "\n",
    "x_ = pd.DataFrame(x_)\n",
    "y_ = pd.DataFrame(y_)\n",
    "\n",
    "data_name.append('Manual F100_I60_R20')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "x_, y_ = make_classification(n_samples=5000, n_features=100,\n",
    "                             n_informative=80, n_redundant=20,\n",
    "                             n_classes=2, random_state=0, shuffle=True)\n",
    "\n",
    "x_ = pd.DataFrame(x_)\n",
    "y_ = pd.DataFrame(y_)\n",
    "\n",
    "data_name.append('Manual F100_I80_R20')\n",
    "x_set.append(x_)\n",
    "y_set.append(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e49942",
   "metadata": {},
   "source": [
    "## 將index轉換為欄位名稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f19ffb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_colname(indexs, oridata):\n",
    "    colname = []\n",
    "    for i in indexs:\n",
    "        colname.append(list(x_.columns)[i])\n",
    "    \n",
    "    return(colname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e3569",
   "metadata": {},
   "source": [
    "## 輸入模型將屬性重要性轉為index輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108c88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_to_index(feature_importances_, prob):\n",
    "    importances = list(feature_importances_)\n",
    "    s = importances\n",
    "    s2 = sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
    "    return s2[:int(len(s2)*prob)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3baf14",
   "metadata": {},
   "source": [
    "## 交叉驗證AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae47678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算AUC函數\n",
    "def get_auc(model, x, y):\n",
    "    \n",
    "    '''\n",
    "    model : 用甚麼方式建模\n",
    "    x : 輸入屬性\n",
    "    y : 目標屬性\n",
    "    '''\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    #建模\n",
    "    #model.fit(x, y)\n",
    "\n",
    "    #用訓練資料測試\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, model.predict(x))\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    print('這次的AUC : ', auc,'\\n')\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806dc5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算f1函數\n",
    "def get_f1(model, x, y):\n",
    "    \n",
    "    '''\n",
    "    model : 用甚麼方式建模\n",
    "    x : 輸入屬性\n",
    "    y : 目標屬性\n",
    "    '''\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    #建模\n",
    "    #model.fit(x, y)\n",
    "\n",
    "    #用訓練資料測試\n",
    "    f1 = f1_score(y, model.predict(x), average='binary')\n",
    "\n",
    "    print('這次的f1 : ', f1,'\\n')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de22f2",
   "metadata": {},
   "source": [
    "## 我的方法特徵選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f7e4f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這次的f1 :  0.6614987080103358 \n",
      "\n",
      "這次的AUC :  0.8241805220011849 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這次的f1 :  0.6529492455418382 \n",
      "\n",
      "這次的AUC :  0.8042383412050274 \n",
      "\n",
      "All_auc :  [0.8241805220011849, 0.8042383412050274]\n",
      "All_f1 :  [0.6614987080103358, 0.6529492455418382]\n",
      "=== churn.all : entropy ===\n",
      "[0.8142094316031061]\n",
      "==================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這次的f1 :  0.7119113573407203 \n",
      "\n",
      "這次的AUC :  0.8379397565085217 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這次的f1 :  0.6942857142857143 \n",
      "\n",
      "這次的AUC :  0.8192222029159492 \n",
      "\n",
      "All_auc :  [0.8379397565085217, 0.8192222029159492]\n",
      "All_f1 :  [0.7119113573407203, 0.6942857142857143]\n",
      "=== churn.all : gini ===\n",
      "[0.8285809797122354]\n",
      "==================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這次的f1 :  0.6204188481675392 \n",
      "\n",
      "這次的AUC :  0.7951723928638814 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這次的f1 :  0.5582047685834501 \n",
      "\n",
      "這次的AUC :  0.7437947883593704 \n",
      "\n",
      "All_auc :  [0.7951723928638814, 0.7437947883593704]\n",
      "All_f1 :  [0.6204188481675392, 0.5582047685834501]\n",
      "=== churn.all : rf_entropy ===\n",
      "[0.769483590611626]\n",
      "==================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這次的f1 :  0.6064880112834979 \n",
      "\n",
      "這次的AUC :  0.7716960618347493 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這次的f1 :  0.5845737483085252 \n",
      "\n",
      "這次的AUC :  0.7657091632836812 \n",
      "\n",
      "All_auc :  [0.7716960618347493, 0.7657091632836812]\n",
      "All_f1 :  [0.6064880112834979, 0.5845737483085252]\n",
      "=== churn.all : rf_gini ===\n",
      "[0.7687026125592152]\n",
      "==================================\n",
      "\n",
      "[17:40:34] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:40:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "這次的f1 :  0.6221009549795361 \n",
      "\n",
      "這次的AUC :  0.7875479455488981 \n",
      "\n",
      "[17:40:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:40:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "這次的f1 :  0.6816269284712483 \n",
      "\n",
      "這次的AUC :  0.8161933119560237 \n",
      "\n",
      "All_auc :  [0.7875479455488981, 0.8161933119560237]\n",
      "All_f1 :  [0.6221009549795361, 0.6816269284712483]\n",
      "=== churn.all : xgb ===\n",
      "[0.8018706287524608]\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifierZe\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#交叉驗證，分層隨機抽樣\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random as rd\n",
    "\n",
    "#依序跑過個資料集\n",
    "for dataname, x_, y_ in zip(data_name, x_set, y_set):\n",
    "\n",
    "\n",
    "    #'corr', 'var'：測試階段\n",
    "    fs_method = ['entropy', 'gini', 'rf_entropy', 'rf_gini', 'xgb']\n",
    "\n",
    "    for fsMethod in fs_method:\n",
    "\n",
    "        f1 = [] #存不同方法的交叉驗證平均f1\n",
    "        AUC = [] #存不同方法的交叉驗證平均AUC\n",
    "\n",
    "        ans_auc = [] #儲存每次交叉驗證的AUC\n",
    "        ans_f1 = [] #儲存每次交叉驗證的f1\n",
    "\n",
    "        rand = rd.randint(1, 10) #產生交叉驗證抽樣隨機種子\n",
    "        skf = StratifiedKFold(n_splits=10, random_state = rand, shuffle=True)\n",
    "\n",
    "        for train_index, test_index in skf.split(x_, y_):\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index,'\\n')\n",
    "            x_train, x_test = x_.iloc[train_index], x_.iloc[test_index]\n",
    "            y_train, y_test = y_.iloc[train_index], y_.iloc[test_index]\n",
    "\n",
    "            x_train = pd.DataFrame(x_train)\n",
    "            y_train = pd.DataFrame(y_train)\n",
    "\n",
    "            x_test = pd.DataFrame(x_test)\n",
    "            y_test = pd.DataFrame(y_test)   \n",
    "\n",
    "            #選擇建模方法\n",
    "            clf = AdaBoostClassifierZe(\n",
    "                        base_estimator=LogisticRegression(n_jobs = -1), \n",
    "                        n_estimators=100, \n",
    "                        random_state=0, \n",
    "                        algorithm='SAMME', \n",
    "                        fs_enable=fsMethod, \n",
    "                        fs_mode=1)\n",
    "\n",
    "            clf.fit(x_train, y_train)\n",
    "            features = importance_to_index(clf.get_my_feature_importance(), 0.2)\n",
    "            \n",
    "#             clf2 = GaussianNB()\n",
    "#             clf2 = LogisticRegression(n_jobs = -1)\n",
    "            clf2 = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "            clf2.fit(x_train[index_to_colname(features, x_train)], y_train)\n",
    "\n",
    "            ans_f1.append(get_f1(clf2,x_test[index_to_colname(features, x_train)], y_test)) #取得用測試資料測試的f1\n",
    "            ans_auc.append(get_auc(clf2,x_test[index_to_colname(features, x_train)], y_test)) #取得用測試資料測試的AUC\n",
    "\n",
    "        #一個方法的十折交叉驗證平均\n",
    "        AUC.append(np.mean(ans_auc))\n",
    "        f1.append(np.mean(ans_f1))\n",
    "        print('All_auc : ', ans_auc)\n",
    "        print('All_f1 : ', ans_f1)\n",
    "\n",
    "        print('===', dataname, ':', fsMethod, '===')\n",
    "        print(AUC)\n",
    "        print(\"==================================\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83cccd1",
   "metadata": {},
   "source": [
    "## Adaboost原方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea657ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifierZe\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#交叉驗證，分層隨機抽樣\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random as rd\n",
    "\n",
    "#依序跑過個資料集\n",
    "for dataname, x_, y_ in zip(data_name, x_set, y_set):\n",
    "\n",
    "    ans_auc = [] #儲存每次交叉驗證的AUC\n",
    "    ans_f1 = [] #儲存每次交叉驗證的f1\n",
    "\n",
    "    rand = rd.randint(1, 10) #產生交叉驗證抽樣隨機種子\n",
    "    skf = StratifiedKFold(n_splits=10, random_state = rand, shuffle=True)\n",
    "\n",
    "    for train_index, test_index in skf.split(x_, y_):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index,'\\n')\n",
    "        x_train, x_test = x_.iloc[train_index], x_.iloc[test_index]\n",
    "        y_train, y_test = y_.iloc[train_index], y_.iloc[test_index]\n",
    "\n",
    "        x_train = pd.DataFrame(x_train)\n",
    "        y_train = pd.DataFrame(y_train)\n",
    "\n",
    "        x_test = pd.DataFrame(x_test)\n",
    "        y_test = pd.DataFrame(y_test)   \n",
    "\n",
    "\n",
    "        #選擇建模方法\n",
    "        clf = AdaBoostClassifier(n_estimators=100, algorithm='SAMME', random_state=0)\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        importance = clf.feature_importances_\n",
    "        features = importance_to_index(importance, 0.2)\n",
    "#         clf2 = GaussianNB()\n",
    "#         clf2 = LogisticRegression(n_jobs = -1)\n",
    "        clf2 = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "        clf2.fit(x_train[index_to_colname(features, x_train)], y_train)\n",
    "\n",
    "        ans_f1.append(get_f1(clf2,x_test[index_to_colname(features, x_train)], y_test)) #取得用測試資料測試的f1\n",
    "        ans_auc.append(get_auc(clf2,x_test[index_to_colname(features, x_train)], y_test)) #取得用測試資料測試的AUC\n",
    "\n",
    "    #一個方法的十折交叉驗證平均\n",
    "    print(dataname, '_AUC : ', np.mean(ans_auc))\n",
    "    # print('ans_f1 : ', np.mean(ans_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f01d54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
